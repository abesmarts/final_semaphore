# # Input configuration - receives data from multiple sources
# input {
#   # HTTP input for Python monitoring scripts
#   http {
#     port => 5000
#     codec => json
#     tags => ["python_monitoring"]
#   }
  
#   # Beats input for Filebeat logs
#   beats {
#     port => 5044
#     tags => ["filebeat_logs"]
#   }
  
#   # TCP input for structured data
#   tcp {
#     port => 5044
#     codec => json_lines
#     tags => ["tcp_monitoring"]
#   }
#   # 
#   # UDP input for high-frequency data
#   udp {
#     port => 5000
#     codec => json
#     tags => ["udp_metrics"]
#   }
# }

# # Filter configuration - processes and enriches data
# filter {
#   # Add timestamp if not present
#   if ![timestamp] and ![@timestamp] {
#     mutate {
#       add_field => { "timestamp" => "%{@timestamp}" }
#     }
#   }
  
#   # Process Python monitoring data
#   if "python_monitoring" in [tags] {
#     mutate {
#       add_field => { "data_source" => "python_script" }
#     }
    
#     # Handle system monitoring data
#     if [metric_type] == "system_status" {
#       mutate {
#         add_field => { "data_type" => "system_monitoring" }
#       }
      
#       # Convert numeric fields safely
#       if [cpu_percent] {
#         mutate {
#           convert => { "cpu_percent" => "float" }
#         }
#       }
      
#       if [mem_percent] {
#         mutate {
#           convert => { "mem_percent" => "float" }
#         }
#       }
      
#       if [disk_percent] {
#         mutate {
#           convert => { "disk_percent" => "float" }
#         }
#       }
      
#       # Add alert levels based on thresholds
#       ruby {
#         code => '
#           cpu = event.get("cpu_percent").to_f
#           mem = event.get("mem_percent").to_f
#           disk = event.get("disk_percent").to_f
          
#           max_usage = [cpu, mem, disk].max
          
#           if max_usage >= 95
#             event.set("alert_level", "critical")
#           elsif max_usage >= 80
#             event.set("alert_level", "warning")
#           else
#             event.set("alert_level", "normal")
#           end
#         '
#       }
#     }
    
#     # Handle web automation data
#     if [metric_type] == "web_automation" {
#       mutate {
#         add_field => { "data_type" => "web_automation" }
#       }
      
#       # Categorize response times
#       if [response_time_seconds] {
#         ruby {
#           code => '
#             response_time = event.get("response_time_seconds").to_f
            
#             if response_time < 2
#               event.set("response_category", "fast")
#             elsif response_time < 5
#               event.set("response_category", "normal")
#             elsif response_time < 10
#               event.set("response_category", "slow")
#             else
#               event.set("response_category", "very_slow")
#             end
#           '
#         }
#       }
      
#       # Extract domain from URL
#       if [website_url] {
#         grok {
#           match => { "website_url" => "https?://(?<domain>[^/]+)" }
#           tag_on_failure => ["_grokparsefailure_domain"]
#         }
#       }
      
#       # Add success/failure status
#       if [success] == true {
#         mutate { add_field => { "status" => "success" } }
#       } else {
#         mutate { add_field => { "status" => "failure" } }
#       }
#     }
#   }
  
#   # Process Filebeat logs
#   if "filebeat_logs" in [tags] {
#     mutate {
#       add_field => { 
#         "data_source" => "filebeat"
#         "data_type" => "system_logs"
#       }
#     }
#   }
  
#   # Global enrichment
#   mutate {
#     add_field => { 
#       "processed_at" => "%{@timestamp}"
#       "pipeline_version" => "2.0"
#       "stack_component" => "logstash"
#     }
#   }
  
#   # Remove unnecessary fields
#   mutate {
#     remove_field => [ "headers", "user_agent", "@version" ]
#   }
# }

# # Output configuration - sends processed data to Elasticsearch
# output {
#   # Route system monitoring data
#   if [data_type] == "system_monitoring" {
#     elasticsearch {
#       hosts => ["elasticsearch:9200"]
#       index => "system-monitoring-%{+YYYY.MM.dd}"
#       template_name => "system_monitoring_template"
#       template_pattern => "system-monitoring-*"
#       template => {
#         "index_patterns" => ["system-monitoring-*"]
#         "settings" => {
#           "number_of_shards" => 1
#           "number_of_replicas" => 0
#           "refresh_interval" => "30s"
#         }
#         "mappings" => {
#           "properties" => {
#             "@timestamp" => { "type" => "date" }
#             "host" => { "type" => "keyword" }
#             "cpu_percent" => { "type" => "float" }
#             "mem_percent" => { "type" => "float" }
#             "disk_percent" => { "type" => "float" }
#             "alert_level" => { "type" => "keyword" }
#             "environment" => { "type" => "keyword" }
#           }
#         }
#       }
#     }
#   }
  
#   # Route web automation data
#   else if [data_type] == "web_automation" {
#     elasticsearch {
#       hosts => ["elasticsearch:9200"]
#       index => "web-automation-%{+YYYY.MM.dd}"
#       template_name => "web_automation_template"
#       template_pattern => "web-automation-*"
#       template => {
#         "index_patterns" => ["web-automation-*"]
#         "settings" => {
#           "number_of_shards" => 1
#           "number_of_replicas" => 0
#         }
#         "mappings" => {
#           "properties" => {
#             "@timestamp" => { "type" => "date" }
#             "website" => { "type" => "keyword" }
#             "domain" => { "type" => "keyword" }
#             "success" => { "type" => "boolean" }
#             "response_time_seconds" => { "type" => "float" }
#             "response_category" => { "type" => "keyword" }
#             "status" => { "type" => "keyword" }
#             "error_message" => { "type" => "text" }
#           }
#         }
#       }
#     }
#   }
  
#   # Route system logs from Filebeat
#   else if [data_type] == "system_logs" {
#     elasticsearch {
#       hosts => ["elasticsearch:9200"]
#       index => "system-logs-%{+YYYY.MM.dd}"
#     }
#   }
  
#   # Default routing for other data
#   else {
#     elasticsearch {
#       hosts => ["elasticsearch:9200"]
#       index => "monitoring-%{+YYYY.MM.dd}"
#     }
#   }
  
#   # Send critical alerts to separate index
#   if [alert_level] == "critical" {
#     elasticsearch {
#       hosts => ["elasticsearch:9200"]
#       index => "alerts-%{+YYYY.MM.dd}"
#     }
#   }
  
#   # Debug output
#   stdout {
#     codec => rubydebug {
#       metadata => true
#     }
#   }
# }


# Logstash Pipeline Configuration
input {
  beats {
    port => 5044
  }
}

filter {
  # Parse Docker container logs
  if [container][name] {
    mutate {
      add_field => { "service_name" => "%{[container][name]}" }
    }
  }

  # Parse Semaphore specific logs
  if [container][name] == "semaphore" {
    grok {
      match => { 
        "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] %{LOGLEVEL:level} %{GREEDYDATA:log_message}"
      }
    }
    
    # Parse task logs
    if [log][file][path] =~ /tasks\.log/ {
      mutate {
        add_field => { "log_type" => "semaphore_task" }
      }
    }
    
    # Parse event logs
    if [log][file][path] =~ /events\.log/ {
      mutate {
        add_field => { "log_type" => "semaphore_event" }
      }
    }
  }

  # Parse MySQL logs
  if [container][name] == "mysql" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{NUMBER:thread_id} \[%{DATA:level}\] %{GREEDYDATA:log_message}"
      }
    }
    mutate {
      add_field => { "log_type" => "mysql" }
    }
  }

  # Parse Elasticsearch logs
  if [container][name] == "elasticsearch" {
    grok {
      match => { 
        "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{LOGLEVEL:level}\s*\]\[%{DATA:component}\] %{GREEDYDATA:log_message}"
      }
    }
    mutate {
      add_field => { "log_type" => "elasticsearch" }
    }
  }

  # Parse Kibana logs
  if [container][name] == "kibana" {
    json {
      source => "message"
    }
    mutate {
      add_field => { "log_type" => "kibana" }
    }
  }

  # Add common fields
  mutate {
    add_field => { "environment" => "development" }
    add_field => { "project" => "semaphore-elk" }
  }

  # Clean up fields
  mutate {
    remove_field => ["agent", "ecs", "host", "input"]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "semaphore-logs-%{+YYYY.MM.dd}"
    template_name => "semaphore-logs"
    template => {
      "index_patterns" => ["semaphore-logs-*"],
      "settings" => {
        "number_of_shards" => 1,
        "number_of_replicas" => 0
      },
      "mappings" => {
        "properties" => {
          "@timestamp" => { "type" => "date" },
          "message" => { "type" => "text" },
          "level" => { "type" => "keyword" },
          "service_name" => { "type" => "keyword" },
          "log_type" => { "type" => "keyword" },
          "container" => {
            "properties" => {
              "name" => { "type" => "keyword" },
              "id" => { "type" => "keyword" }
            }
          }
        }
      }
    }
  }
  
  # Debug output
  stdout {
    codec => rubydebug
  }
}
